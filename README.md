# ğŸ§  AI Chat App (Electron + Local LLM)

A local chat AI app built with Electron.js and styled like LM Studio. Chat with LLaMA models (GGUF) **fully offline**, with slick UI and tuning options.

---

## âœ¨ Features
- ğŸ—‚ï¸ Model picker (.gguf)
- ğŸ§  System prompt customization
- ğŸ”§ Adjustable temperature, top-k, top-p
- ğŸ“¡ Local server start/stop
- ğŸ“œ Server logs viewer
- ğŸ’¬ Chat history & new chat tab
- ğŸŒ— Light/Dark mode toggle
- ğŸ¯ Clean, minimal UI inspired by OpenAI Chat

---

## ğŸ–¥ï¸ Tech Stack
- Electron.js
- HTML + TailwindCSS
- Node.js
- Local LLaMA models via `llama-server`

---

## âš™ï¸ Setup Instructions

### 1. Clone the repo
```bash
git clone https://github.com/MrNuby/ai-chat-electron.git
cd ai-chat-electron

### 2. Install dependencies
```bash
npm install

### 3. Start the app
```bash
npm start

---

## ğŸ§ª How it works
Starts a local llama-server instance (via child_process)

Communicates with the API on http://localhost:11434/v1/chat/completions

Uses streaming to show tokens as they arrive

UI handles model configs, settings modal, and dynamic tabs